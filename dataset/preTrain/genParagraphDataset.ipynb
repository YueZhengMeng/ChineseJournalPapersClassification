{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "seed = 3407"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "3407"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "seed_everything(seed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "36"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "paragraphs = []\n",
    "p = []\n",
    "with open('paragraph.txt', 'r', encoding='UTF-8') as f:\n",
    "    total_data = f.readlines()\n",
    "    with tqdm(total_data) as loader:\n",
    "        for data in loader:\n",
    "            if data == \"#*#\" + '\\n':\n",
    "                paragraphs.append(p.copy())\n",
    "                p.clear()\n",
    "            else:\n",
    "                p.append(data)\n",
    "\n",
    "len(paragraphs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_next_sentence_sample(sentence, next_sentence, paragraphs, nsp_probability=0.5):\n",
    "   if random.random() < nsp_probability:\n",
    "       labels = 1\n",
    "   else:\n",
    "       next_sentence = random.choice(random.choice(paragraphs))\n",
    "       labels = 0\n",
    "   return sentence, next_sentence, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 35925.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'next_sentence', 'labels'],\n    num_rows: 75\n})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dict_dataset = {\"sentence\":[],\"next_sentence\":[],\"labels\":[]}\n",
    "for p in tqdm(paragraphs):\n",
    "    for i in range(len(p) - 1):  # 遍历一个段落中的每一句话\n",
    "        sentence, next_sentence, labels = get_next_sentence_sample(p[i], p[i + 1], paragraphs)  # 构造NSP样本\n",
    "        if len(sentence)>=500:\n",
    "            continue\n",
    "        else:\n",
    "            dict_dataset[\"sentence\"].append(sentence)\n",
    "            dict_dataset[\"next_sentence\"].append(next_sentence)\n",
    "            dict_dataset[\"labels\"].append(labels)\n",
    "\n",
    "dataset = Dataset.from_dict(dict_dataset)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/vocab.txt (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000016CBBC2A4F0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "model_path = 'bert-base-chinese'\n",
    "vocab_path = model_path\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenization at 0x0000016CC22B74C0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'next_sentence', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 75\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"sentence\"],example[\"next_sentence\"],truncation=True, padding=\"max_length\",max_length=max_length)\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 75\n})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.remove_columns([\"sentence\",\"next_sentence\"])\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 75\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\",\"labels\"])\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 74\n    })\n    test: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1\n    })\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset.train_test_split(test_size=0.01,shuffle=True,seed=seed)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./dataset_paragraphs\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
