{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    DataCollatorForWholeWordMask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 3407\n",
    "seed_everything(seed)\n",
    "model_path = 'bert-base-chinese'\n",
    "vocab_path = model_path\n",
    "file_path = \"./longStn.txt\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "max_length = 512\n",
    "mlm_probability = 0.15\n",
    "wwm=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = {\"sentence\": []}\n",
    "with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "    lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "    for line in tqdm(lines):\n",
    "        train_data[\"sentence\"].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence'],\n    num_rows: 49\n})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict(train_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/vocab.txt (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001CE8C99BD60>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x000001CE8CA6BA60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'input_ids'],\n    num_rows: 49\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda example:{\"input_ids\":tokenizer(example[\"sentence\"], truncation=True, max_length=max_length).input_ids},batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"./dataset_temp_long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'input_ids'],\n    num_rows: 49\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"./dataset_temp_long\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:05<?, ?ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence', 'input_ids', 'chinese_ref'],\n    num_rows: 49\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from get_chinese_ref import prepare_ref\n",
    "from ltp import LTP\n",
    "\n",
    "if wwm:\n",
    "    ltp = LTP().to(device)#这一句要加载180MB的模型并初始化，所以很慢，要大概五秒钟时间\n",
    "    dataset = dataset.map(lambda example:{\"chinese_ref\":prepare_ref(example[\"sentence\"], ltp, tokenizer)},batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'谷物联合收获机自动测产系统设计-基于变权分层激活扩散模型[PAD]联合收割机[PAD]测产系统[PAD]变权分层[PAD]激活扩散[PAD]为了使联合收割机具有自动测产功能，提出了一种基于变权分层激活扩散的产量预测误差剔除模型，并使用单片机设计了联合收获机测产系统。测产系统的主要功能是：在田间进行作业时，收割机可以测出当前的运行速度、收获面积及谷物的总体产量。数据的采集使用霍尔传感器和电容压力传感器，具有较高的精度。模拟信号的处理选用了 ADC0804差分式 A／D转换芯片，可以有效地克服系统误差，数据传送到单片机处理中心，对每一次转换都进行一次判断，利用变权分层激活扩散模型剔除误差较大的数据，通过计算将数据最终在LCD显示屏进行显示。将系统应用在了收割机上，通过测试得到了谷物产量的测量值，并与真实值进行比较，验证了系统的可靠性。'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"sentence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] 谷 物 联 合 收 获 机 自 动 测 产 系 统 设 计 - 基 于 变 权 分 层 激 活 扩 散 模 型 [PAD] 联 合 收 割 机 [PAD] 测 产 系 统 [PAD] 变 权 分 层 [PAD] 激 活 扩 散 [PAD] 为 了 使 联 合 收 割 机 具 有 自 动 测 产 功 能 ， 提 出 了 一 种 基 于 变 权 分 层 激 活 扩 散 的 产 量 预 测 误 差 剔 除 模 型 ， 并 使 用 单 片 机 设 计 了 联 合 收 获 机 测 产 系 统 。 测 产 系 统 的 主 要 功 能 是 ： 在 田 间 进 行 作 业 时 ， 收 割 机 可 以 测 出 当 前 的 运 行 速 度 、 收 获 面 积 及 谷 物 的 总 体 产 量 。 数 据 的 采 集 使 用 霍 尔 传 感 器 和 电 容 压 力 传 感 器 ， 具 有 较 高 的 精 度 。 模 拟 信 号 的 处 理 选 用 了 [UNK] 差 分 式 [UNK] ／ [UNK] 转 换 芯 片 ， 可 以 有 效 地 克 服 系 统 误 差 ， 数 据 传 送 到 单 片 机 处 理 中 心 ， 对 每 一 次 转 换 都 进 行 一 次 判 断 ， 利 用 变 权 分 层 激 活 扩 散 模 型 剔 除 误 差 较 大 的 数 据 ， 通 过 计 算 将 数 据 最 终 在 [UNK] 显 示 屏 进 行 显 示 。 将 系 统 应 用 在 了 收 割 机 上 ， 通 过 测 试 得 到 了 谷 物 产 量 的 测 量 值 ， 并 与 真 实 值 进 行 比 较 ， 验 证 了 系 统 的 可 靠 性 。 [SEP]'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[2,\n 4,\n 6,\n 7,\n 9,\n 11,\n 13,\n 15,\n 18,\n 20,\n 22,\n 24,\n 25,\n 26,\n 28,\n 31,\n 33,\n 34,\n 37,\n 39,\n 42,\n 44,\n 47,\n 48,\n 49,\n 52,\n 55,\n 57,\n 58,\n 60,\n 62,\n 64,\n 66,\n 69,\n 74,\n 76,\n 78,\n 80,\n 81,\n 82,\n 85,\n 87,\n 89,\n 91,\n 93,\n 97,\n 99,\n 100,\n 102,\n 105,\n 107,\n 108,\n 110,\n 112,\n 115,\n 117,\n 120,\n 122,\n 127,\n 129,\n 131,\n 135,\n 136,\n 138,\n 140,\n 142,\n 145,\n 147,\n 150,\n 152,\n 155,\n 158,\n 160,\n 163,\n 166,\n 168,\n 170,\n 172,\n 173,\n 176,\n 178,\n 180,\n 181,\n 184,\n 189,\n 192,\n 194,\n 197,\n 199,\n 209,\n 211,\n 214,\n 216,\n 219,\n 221,\n 223,\n 226,\n 228,\n 231,\n 232,\n 234,\n 236,\n 243,\n 246,\n 250,\n 253,\n 255,\n 257,\n 259,\n 260,\n 261,\n 263,\n 265,\n 267,\n 272,\n 275,\n 277,\n 280,\n 282,\n 286,\n 287,\n 289,\n 291,\n 295,\n 297,\n 301,\n 302,\n 306,\n 308,\n 310,\n 313,\n 314,\n 315,\n 318,\n 319,\n 323,\n 325,\n 327,\n 329,\n 332,\n 335,\n 338,\n 339]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"chinese_ref\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'[UNK] [UNK] [UNK] [UNK] [UNK] 收 获 机 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 激 活 扩 散 模 型 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 测 产 [UNK] [UNK] [UNK] 变 权 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 使 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 了 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 了 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 收 割 机 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 。 [UNK] [UNK] 的 [UNK] [UNK] [UNK] [UNK] 霍 尔 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 压 力 传 感 器 [UNK] [UNK] [UNK] [UNK] 高 的 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ／ [UNK] 转 换 [UNK] [UNK] [UNK] [UNK] [UNK] 有 效 [UNK] 克 服 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 误 差 [UNK] [UNK] 的 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 显 示 屏 [UNK] [UNK] 显 示 [UNK] 将 [UNK] [UNK] [UNK] [UNK] [UNK] 了 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 得 到 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 。 [UNK]'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator([dataset[0]])[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"./dataset_long\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
